{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "19CH30018_Assn1_NLP_A21.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Z_wN2v1RT1F"
      },
      "source": [
        "# **Assignment-1 for CS60075: Natural Language Processing**\n",
        "\n",
        "#### Instructor : Prof. Sudeshna Sarkar\n",
        "\n",
        "#### Teaching Assistants : Alapan Kuila, Aniruddha Roy, Prithwish Jana, Udit Dharmin Desai\n",
        "\n",
        "#### Date of Announcement: 4th Sept, 2021\n",
        "#### Deadline for Submission: 11.59pm on Sunday, 12th Sept, 2021 \n",
        "\n",
        "#### (**NOTE**: Submit a .zip file, containing this .ipynb file, named as `<Your_Roll_Number>_Assn1_NLP_A21.ipynb` and the raw text corpus named `<Your_Roll_Number>_Assn1_rawCorpus.txt`. For example, if your roll number is 20XX12Y45, name the .ipynb file as `20XX12Y45_Assn1_NLP_A21.ipynb`. Name the .zip as `<Your_Roll_Number>_Assn1_NLP_A21.zip`. Write your code in the respective designated portion of the .ipynb. Also before submitting, make sure that all the outputs of your code are present in the .ipynb file itself.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a35tmEySCx7"
      },
      "source": [
        "### **Submission Details:**\n",
        "Name: S.S.S.Vardhan\n",
        "\n",
        "Roll No.: 19CH30018\n",
        "\n",
        "Department: Chemical Engineering\n",
        "\n",
        "Email-ID: vardhanfirstof@gmail.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9weHMmyd8fnq"
      },
      "source": [
        "## **Reading a Raw Text Corpus**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmSy_LOK2aGQ"
      },
      "source": [
        "Retrieve & save raw corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rku6rV2ORpZA"
      },
      "source": [
        "# To construct your corpus, retrieve (through Python code) Chapter I to Chapter X,\n",
        "# both inclusive, from the link below:\n",
        "# \"https://www.gutenberg.org/files/730/730-0.txt\"\n",
        "# Save this corpus in a text file, named as 'rawCorpus.txt'\n",
        "# Print the total number of characters in the text file \n",
        "url=\"https://www.gutenberg.org/files/730/730-0.txt\"\n",
        "# Make the request and check object type\n",
        "# *** Write code ***\n",
        "import requests\n",
        "req = requests.get(url)\n",
        "req.encoding = \"utf-8\"\n",
        "text = req.text"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wF4519bK3XK"
      },
      "source": [
        "mycorpus= text[text.find(\"CHAPTER I\"):text.find(\"CHAPTER XI\")]\n",
        "#print(mycorpus)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4FrVC5meK6f",
        "outputId": "2f90ba7d-35a6-4213-948c-3d0fcbf79a42"
      },
      "source": [
        "with open(\"19CH30018_Assn1_rawCorpus.txt\", \"w\") as text_file:\n",
        "    print(mycorpus, file=text_file)\n",
        "print(\"Total # of characters from scrapping data:\",len(mycorpus))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total # of characters from scrapping data: 151735\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KZIOy0Y2hzQ"
      },
      "source": [
        "Read the corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsdBJa_l2l7g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ce231e0-83a7-46d9-c60c-260165647138"
      },
      "source": [
        "# Read the corpus from rawCorpus.txt, in a variable `rawReadCorpus`\n",
        "# *** Write code ***\n",
        "file = open(\"19CH30018_Assn1_rawCorpus.txt\", \"r\")\n",
        "rawReadCorpus = file.read()\n",
        "\n",
        "print (\"Total # of characters in read dataset: {}\".format(len(rawReadCorpus)))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total # of characters in read dataset: 148718\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhkmGsSoV0zG"
      },
      "source": [
        "## **Installing NLTK**\n",
        "\n",
        "The Natural Language Toolkit ([NLTK](https://www.nltk.org/)) is a Python module that is intended to support research and teaching in NLP or closely related areas. \n",
        "\n",
        "Detailed installation instructions to install NLTK can be found at this [link](https://www.nltk.org/install.html).\n",
        "\n",
        "To ensure uniformity, we suggest to use **python3**. You can download Anaconda3 and create a separate environment to do this assignment, eg.\n",
        "```bash\n",
        "conda create -n myenv python=3.6\n",
        "conda activate myenv\n",
        "```\n",
        "\n",
        "The link to anaconda3 for Windows and Linux is available here https://docs.anaconda.com/anaconda/install/. Subsequently, you can install NLTK through the following commands:\n",
        "```bash\n",
        "sudo pip3 install nltk \n",
        "python3 \n",
        "nltk.download()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AciN3dYgxlSX"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpNRAa-cw3RW",
        "outputId": "6a06b6fc-ee30-4df9-89bd-97d94781fa97"
      },
      "source": [
        "if (nltk.__version__ == '3.4'):\n",
        "  print(\"nltk Version 3.4\")\n",
        "else:\n",
        "  print(\"Use pip uninstall nltk\")\n",
        "  print(\"Then, pip install nltk==3.4\")\n",
        "  print(\"Finally, restart the kernel and run it!\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nltk Version 3.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utKtZeHq4N98"
      },
      "source": [
        "## **Preprocessing the corpus**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-LSUX__82Ff"
      },
      "source": [
        "**Tokenize into words and sentences, using NLTK library:** Using the NLTK modules imported above, retrieve a case-insensitive preprocessed model. Make sure to take care of words like \"\\_will\\_\" (that should ideally appear as \"will\"), \"wouldn't\" (that should ideally appear as a single word, and not multiple tokens) and other occurences of special cases that you find in the raw corpus. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2g7eO4Dm4jIn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e504e131-4b2c-4fd1-b6bf-4d41309633d2"
      },
      "source": [
        "# Importing modules\n",
        "nltk.download('punkt') # For tokenizers\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XrKxwxCJwN7"
      },
      "source": [
        "def tokenized_words(rawReadCorpus):\n",
        "  corpus = word_tokenize(rawReadCorpus)\n",
        "  count = 0\n",
        "  while count<len(corpus):\n",
        "    word = corpus[count]\n",
        "    corpus[count]= word.lower()\n",
        "    if \"'\" in word:\n",
        "      corpus[count]=corpus[count-1]+corpus[count]\n",
        "      corpus.pop(count-1)\n",
        "      count-=1\n",
        "    if '_' in word:\n",
        "      corpus[count]=word[1:len(word)-1]\n",
        "    count+=1\n",
        "\n",
        "  final_corpus=[]\n",
        "  for word in corpus:\n",
        "    for letter in word:\n",
        "      if not (letter>=\"a\" and letter<=\"z\"):\n",
        "        word = word.replace(letter,'')\n",
        "    if word!='':\n",
        "      final_corpus.append(word)\n",
        "  return final_corpus"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESsgCRRtl3hY"
      },
      "source": [
        "def tokenized_sentences(rawReadCorpus):\n",
        "  rawReadCorpus=rawReadCorpus.replace(\"!\",\".\").replace(\"?\",\".\").replace(\",\",\" \").replace('”',\" \").replace(\":\",\" \").replace(\";\",\" \").replace(\"  —  \",' ').replace(\"_\",\" \").replace(\"(\",\".\").replace(\")\",\".\")\n",
        "  sentences=sent_tokenize(rawReadCorpus)\n",
        "  final_sentence=[]\n",
        "  final_sentences=[]\n",
        "  for sentence in sentences:\n",
        "    sentence = sentence.lower()\n",
        "    if \"\\n\\n\" in sentence:\n",
        "      sentence=sentence.replace(\"\\n\\n\",\". \")\n",
        "      for sub_sentence in sent_tokenize(sentence):\n",
        "        final_sentence.append(sub_sentence.replace(\"\\n\",\" \"))\n",
        "    else:\n",
        "      final_sentence.append(sentence.replace(\"\\n\",\" \"))\n",
        "  for sentence in final_sentence:\n",
        "      if \"chapter i\" in sentence or \"chapter v\" in sentence or \"chapter x\" in sentence:\n",
        "        pass\n",
        "      else:\n",
        "        if sentence.count('  ')>=1:\n",
        "          sentence = sentence.replace('  ',' ')\n",
        "        final_sentences.append(sentence.replace(\".”\",\" \").replace(\".\",\" \").replace('”',' '))\n",
        "  return final_sentences"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWIzYXyz9Zt_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61d84c75-d181-4ce2-9b3d-b9720f53905e"
      },
      "source": [
        "# *** Write code for preprocessing the corpus ***\n",
        "import re\n",
        "rawReadCorpus = rawReadCorpus.replace(\"’\",\"'\").replace(\"—\",\" — \")\n",
        "rawReadCorpus = re.sub(\"CHAPTER \\w+[.]\",\"\", rawReadCorpus)\n",
        "\n",
        "corpus_words= tokenized_words(rawReadCorpus)\n",
        "corpus_sentences = tokenized_sentences(rawReadCorpus)\n",
        "average_tokens = round(len(corpus_words)/len(corpus_sentences))\n",
        "unique_tokens = set(corpus_words)\n",
        "\n",
        "print(\"The number of sentences is\", len(corpus_sentences))\n",
        "print(\"The number of tokens is\", len(corpus_words)) \n",
        "\n",
        "#prints the average number of tokens per sentence\n",
        "print(\"The average number of tokens per sentence is\",average_tokens) \n",
        "#prints the number of unique tokens\n",
        "print(\"The number of unique tokens are\", len(unique_tokens)) \n",
        "# Print first 5 sentences of your preprocessed corpus *** Write code ***\n",
        "print(\"First 5 sentences\",corpus_sentences[0:5])\n",
        "# Print first 5 words/tokens of your preprocessed corpus *** Write code ***\n",
        "print(\"First 5 words\",corpus_words[0:5])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of sentences is 1462\n",
            "The number of tokens is 26075\n",
            "The average number of tokens per sentence is 18\n",
            "The number of unique tokens are 4280\n",
            "First 5 sentences [' treats of the place where oliver twist was born and of the circumstances attending his birth ', 'among other public buildings in a certain town which for many reasons it will be prudent to refrain from mentioning and to which i will assign no fictitious name there is one anciently common to most towns great or small to wit a workhouse and in this workhouse was born on a day and date which i need not trouble myself to repeat inasmuch as it can be of no possible consequence to the reader in this stage of the business at all events the item of mortality whose name is prefixed to the head of this chapter ', 'for a long time after it was ushered into this world of sorrow and trouble by the parish surgeon it remained a matter of considerable doubt whether the child would survive to bear any name at all in which case it is somewhat more than probable that these memoirs would never have appeared or if they had that being comprised within a couple of pages they would have possessed the inestimable merit of being the most concise and faithful specimen of biography extant in the literature of any age or country ', 'although i am not disposed to maintain that the being born in a workhouse is in itself the most fortunate and enviable circumstance that can possibly befall a human being i do mean to say that in this particular instance it was the best thing for oliver twist that could by possibility have occurred ', 'the fact is that there was considerable difficulty in inducing oliver to take upon himself the office of respiration — a troublesome practice but one which custom has rendered necessary to our easy existence and for some time he lay gasping on a little flock mattress rather unequally poised between this world and the next the balance being decidedly in favour of the latter ']\n",
            "First 5 words ['treats', 'of', 'the', 'place', 'where']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ75_a1QL70J"
      },
      "source": [
        "**Perform the following tasks for the given corpus:**\n",
        "1. Print the average number of tokens per sentence.\n",
        "2. Print the length of the longest and the shortest sentence, that contains the word 'Oliver' ('Oliver' is case-insensitive).\n",
        "3. Print the number of unique tokens in the corpus, after stopword removal using the stopwords from NLTK (case-insensitive)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyG0g3oSADmV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89d99e5f-37b0-4cd2-a556-1c148b4c2db2"
      },
      "source": [
        "# Importing modules\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydHIxC7lG7Py",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbfc82d4-726e-430b-c983-21c56791ca7b"
      },
      "source": [
        "# *** Write code for the 2 tasks above ***\n",
        "mx_oliver,mn_oliver=0,99999999\n",
        "for i in corpus_sentences:\n",
        "  if 'oliver' in i.lower():\n",
        "    mx_oliver = max(len(i),mx_oliver)\n",
        "    mn_oliver = min(len(i),mn_oliver)\n",
        "print(\"Length of the longest with Oliver(case-insesnitive):\",mx_oliver,\"\\nLength of the shortest with Oliver(case-insesnitive):\",mn_oliver)\n",
        "\n",
        "final_tokens=[]\n",
        "for word in corpus_words:\n",
        " if word not in stop_words:\n",
        "    final_tokens.append(word)\n",
        "unique_tokens = set(final_tokens)\n",
        "print(\"The number of total tokens after removing stopwords are\", len((unique_tokens)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of the longest with Oliver(case-insesnitive): 632 \n",
            "Length of the shortest with Oliver(case-insesnitive): 8\n",
            "The number of total tokens after removing stopwords are 4159\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5RiDR7TJjKX"
      },
      "source": [
        "## **Language Modeling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJeTSt8HM95L"
      },
      "source": [
        "### Task: In this sub-task, you are expected to carry out the following tasks:\n",
        "\n",
        "1. **Create the following language models** on the given corpus: <br>\n",
        "    i.   Unigram <br>\n",
        "    ii.  Bigram <br>\n",
        "    iii. Trigram <br>\n",
        "\n",
        "2. **List the top 10 bigrams, trigrams**\n",
        "(Additionally remove those items which contain only articles, prepositions, determiners eg. \"of the\", \"in a\", etc. List top-10 bigrams/trigrams in both the original and processed models)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlPXGvVaR-ka",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f361dd48-9697-4939-f477-a31adbd04e73"
      },
      "source": [
        "from nltk.util import ngrams\n",
        "unigram=[]\n",
        "bigram=[]\n",
        "trigram=[]\n",
        "\n",
        "for content in corpus_sentences:\n",
        "  word = tokenized_words(content)\n",
        "  unigram.extend(ngrams(word, 1))\n",
        "for content in corpus_sentences:\n",
        "  words = word_tokenize(content)\n",
        "  while '“' in words:\n",
        "    words.remove('“')\n",
        "  count = 0\n",
        "  while count<len(words):\n",
        "    word = words[count]\n",
        "    words[count]= word.lower()\n",
        "    if \"'\" in word:\n",
        "      words[count]=words[count-1]+words[count]\n",
        "      words.pop(count-1)\n",
        "      count-=1\n",
        "    count+=1\n",
        "  bigram.extend(ngrams(words, 2))\n",
        "  trigram.extend(ngrams(words, 3))\n",
        "    ##similar for trigrams \n",
        "    # *** Write code ***\n",
        "\n",
        "print (\"Sample of n-grams:\\n\" + \"-------------------------\")\n",
        "print (\"--> UNIGRAMS: \\n\" + str(unigram[:5]) + \" ...\\n\")\n",
        "print (\"--> BIGRAMS: \\n\" + str(bigram[:5]) + \" ...\\n\")\n",
        "print (\"--> TRIGRAMS: \\n\" + str(trigram[:5]) + \" ...\\n\")\n",
        "\n",
        "def removal(x):     \n",
        "#removes ngrams containing only stopwords\n",
        "    y = []\n",
        "    for pair in x:\n",
        "        count = 0\n",
        "        for word in pair:\n",
        "            if word in stop_words:\n",
        "                count = count or 0\n",
        "            else:\n",
        "                count = count or 1\n",
        "        if (count==1):\n",
        "            y.append(pair)\n",
        "    return (y)\n",
        "\n",
        "# list of unigram, bigram & trigram after removing those that \n",
        "# totally contain only articles, prepositions, determiners\n",
        "# Eg. For bigrams, don't remove items like (\"a\", \"boy\") --> where not all are \n",
        "#     articles, prepositions, determiners\n",
        "#     But remove items like (\"in\", \"the\") --> where all are articles, prepositions, determiners\n",
        "# Similarly, for unigrams and trigrams\n",
        "unigrams_Processed = removal(unigram)# *** Write code ***\n",
        "bigrams_Processed = removal(bigram)# *** Write code ***\n",
        "trigrams_Processed = removal(trigram) # *** Write code ***\n",
        "\n",
        "print (\"Sample of n-grams after processing:\\n\" + \"-------------------------\")\n",
        "print (\"--> UNIGRAMS: \\n\" + str(unigrams_Processed[:5]) + \" ...\\n\")\n",
        "print (\"--> BIGRAMS: \\n\" + str(bigrams_Processed[:5]) + \" ...\\n\")\n",
        "print (\"--> TRIGRAMS: \\n\" + str(trigrams_Processed[:5]) + \" ...\\n\")\n",
        "\n",
        "def get_ngrams_freqDist(n, ngramList):\n",
        "    #This function computes the frequency corresponding to each ngram in ngramList \n",
        "    #Here, n=1 for unigram, n=2 for bigram, etc.\n",
        "    #ngramList = list of unigrams when n=1, ngramList = list of bigrams when n=2\n",
        "    #Returns: ngram_freq_dict (a Python dictionary where key = a ngram, value = its frequency)\n",
        "    \n",
        "    # *** Write code ***\n",
        "    ngram_freq_dict={}\n",
        "    for word in ngramList:\n",
        "      if word in ngram_freq_dict:\n",
        "        ngram_freq_dict[word]+=1\n",
        "      else :\n",
        "        ngram_freq_dict[word]=1\n",
        "    \n",
        "    return ngram_freq_dict\n",
        "\n",
        "def top_ten(ngram_freq_dict):\n",
        "  return (sorted(ngram_freq_dict.items(), key=lambda item: item[1],reverse=True))[0:10]\n",
        "\n",
        "unigrams_freqDist = get_ngrams_freqDist(1, unigram)\n",
        "unigrams_Processed_freqDist = get_ngrams_freqDist(1, unigrams_Processed)\n",
        "bigrams_freqDist = get_ngrams_freqDist(2, bigram)\n",
        "bigrams_Processed_freqDist = get_ngrams_freqDist(2, bigrams_Processed)\n",
        "trigrams_freqDist = get_ngrams_freqDist(3, trigram)\n",
        "trigrams_Processed_freqDist = get_ngrams_freqDist(3, trigrams_Processed)                                                 \n",
        "\n",
        "from tabulate import tabulate\n",
        "def print_top_ten(top_ten_dict):\n",
        "  table=[[\"Rank\",\"Word\",\"Frequency\"]]\n",
        "  for a,b in top_ten_dict:\n",
        "    table.append([a,b])\n",
        "  print(tabulate(table, headers='firstrow',showindex=range(1,11)))\n",
        "  print(\"-\"*50,\"\\n\")\n",
        "\n",
        "# Print top 10 unigrams, having highest frequency as in unigrams_freqDist\n",
        "# *** Write code ***\n",
        "print(\"Top 10 unigrams, having highest frequency\\n\")\n",
        "print_top_ten(top_ten(unigrams_freqDist))\n",
        "# Print top 10 unigrams, having highest frequency as in unigrams_Processed_freqDist\n",
        "print(\"Top 10 unigrams, having highest frequency, after processing\\n\")\n",
        "print_top_ten(top_ten(unigrams_Processed_freqDist))\n",
        "# Print top 10 bigrams, having highest frequency as in bigrams_freqDist\n",
        "# *** Write code ***\n",
        "print(\"Top 10 bigrams, having highest frequency\\n\")\n",
        "print_top_ten(top_ten(bigrams_freqDist))\n",
        "# Print top 10 bigrams, having highest frequency as in bigrams_Processed_freqDist\n",
        "# *** Write code ***\n",
        "print(\"Top 10 bigrams, having highest frequency, after processing\\n\")\n",
        "print_top_ten(top_ten(bigrams_Processed_freqDist))\n",
        "# Print top 10 trigrams, having highest frequency as in trigrams_freqDist\n",
        "# *** Write code ***\n",
        "print(\"Top 10 trigrams, having highest frequency\\n\")\n",
        "print_top_ten(top_ten(trigrams_freqDist))\n",
        "# Print top 10 trigrams, having highest frequency as in trigrams_Processed_freqDist\n",
        "print(\"Top 10 trigrams, having highest frequency, after processing\\n\")\n",
        "print_top_ten(top_ten(trigrams_Processed_freqDist))\n",
        "# *** Write code ***\n",
        "# *** Write code ***"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample of n-grams:\n",
            "-------------------------\n",
            "--> UNIGRAMS: \n",
            "[('treats',), ('of',), ('the',), ('place',), ('where',)] ...\n",
            "\n",
            "--> BIGRAMS: \n",
            "[('treats', 'of'), ('of', 'the'), ('the', 'place'), ('place', 'where'), ('where', 'oliver')] ...\n",
            "\n",
            "--> TRIGRAMS: \n",
            "[('treats', 'of', 'the'), ('of', 'the', 'place'), ('the', 'place', 'where'), ('place', 'where', 'oliver'), ('where', 'oliver', 'twist')] ...\n",
            "\n",
            "Sample of n-grams after processing:\n",
            "-------------------------\n",
            "--> UNIGRAMS: \n",
            "[('treats',), ('place',), ('oliver',), ('twist',), ('born',)] ...\n",
            "\n",
            "--> BIGRAMS: \n",
            "[('treats', 'of'), ('the', 'place'), ('place', 'where'), ('where', 'oliver'), ('oliver', 'twist')] ...\n",
            "\n",
            "--> TRIGRAMS: \n",
            "[('treats', 'of', 'the'), ('of', 'the', 'place'), ('the', 'place', 'where'), ('place', 'where', 'oliver'), ('where', 'oliver', 'twist')] ...\n",
            "\n",
            "Top 10 unigrams, having highest frequency\n",
            "\n",
            "  Rank  Word           Frequency\n",
            "------  -----------  -----------\n",
            "     1  ('the',)            1706\n",
            "     2  ('and',)             861\n",
            "     3  ('a',)               714\n",
            "     4  ('of',)              670\n",
            "     5  ('to',)              618\n",
            "     6  ('his',)             454\n",
            "     7  ('he',)              452\n",
            "     8  ('in',)              444\n",
            "     9  ('was',)             368\n",
            "    10  ('oliver',)          279\n",
            "-------------------------------------------------- \n",
            "\n",
            "Top 10 unigrams, having highest frequency, after processing\n",
            "\n",
            "  Rank  Word               Frequency\n",
            "------  ---------------  -----------\n",
            "     1  ('oliver',)              279\n",
            "     2  ('said',)                212\n",
            "     3  ('mr',)                  191\n",
            "     4  ('bumble',)              124\n",
            "     5  ('gentleman',)           102\n",
            "     6  ('old',)                  89\n",
            "     7  ('sowerberry',)           79\n",
            "     8  ('would',)                77\n",
            "     9  ('boy',)                  75\n",
            "    10  ('replied',)              74\n",
            "-------------------------------------------------- \n",
            "\n",
            "Top 10 bigrams, having highest frequency\n",
            "\n",
            "  Rank  Word                Frequency\n",
            "------  ----------------  -----------\n",
            "     1  ('of', 'the')             162\n",
            "     2  ('in', 'the')             128\n",
            "     3  ('mr', 'bumble')          108\n",
            "     4  ('to', 'the')              91\n",
            "     5  ('said', 'the')            90\n",
            "     6  ('he', 'had')              68\n",
            "     7  ('he', 'was')              62\n",
            "     8  ('on', 'the')              60\n",
            "     9  ('in', 'a')                55\n",
            "    10  ('the', 'old')             53\n",
            "-------------------------------------------------- \n",
            "\n",
            "Top 10 bigrams, having highest frequency, after processing\n",
            "\n",
            "  Rank  Word                     Frequency\n",
            "------  ---------------------  -----------\n",
            "     1  ('mr', 'bumble')               108\n",
            "     2  ('said', 'the')                 90\n",
            "     3  ('the', 'old')                  53\n",
            "     4  ('old', 'gentleman')            39\n",
            "     5  ('the', 'undertaker')           37\n",
            "     6  ('the', 'boy')                  35\n",
            "     7  ('said', 'mr')                  34\n",
            "     8  ('mrs', 'sowerberry')           34\n",
            "     9  ('the', 'gentleman')            33\n",
            "    10  ('the', 'jew')                  33\n",
            "-------------------------------------------------- \n",
            "\n",
            "Top 10 trigrams, having highest frequency\n",
            "\n",
            "  Rank  Word                             Frequency\n",
            "------  -----------------------------  -----------\n",
            "     1  ('the', 'old', 'gentleman')             29\n",
            "     2  ('gentleman', 'in', 'the')              22\n",
            "     3  ('the', 'gentleman', 'in')              20\n",
            "     4  ('the', 'white', 'waistcoat')           20\n",
            "     5  ('said', 'mr', 'bumble')                19\n",
            "     6  ('in', 'the', 'white')                  18\n",
            "     7  ('said', 'the', 'undertaker')           15\n",
            "     8  ('said', 'the', 'gentleman')            14\n",
            "     9  ('said', 'the', 'jew')                  14\n",
            "    10  ('sir', 'replied', 'oliver')            12\n",
            "-------------------------------------------------- \n",
            "\n",
            "Top 10 trigrams, having highest frequency, after processing\n",
            "\n",
            "  Rank  Word                             Frequency\n",
            "------  -----------------------------  -----------\n",
            "     1  ('the', 'old', 'gentleman')             29\n",
            "     2  ('gentleman', 'in', 'the')              22\n",
            "     3  ('the', 'gentleman', 'in')              20\n",
            "     4  ('the', 'white', 'waistcoat')           20\n",
            "     5  ('said', 'mr', 'bumble')                19\n",
            "     6  ('in', 'the', 'white')                  18\n",
            "     7  ('said', 'the', 'undertaker')           15\n",
            "     8  ('said', 'the', 'gentleman')            14\n",
            "     9  ('said', 'the', 'jew')                  14\n",
            "    10  ('sir', 'replied', 'oliver')            12\n",
            "-------------------------------------------------- \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lqu8nVV7NREo"
      },
      "source": [
        "## **Next three words' Prediction using Smoothed Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2vnIM26b2WA"
      },
      "source": [
        "For a bigram model, add-one smoothing is defined by $P_{Add-1}(w_i|w_{i-1})=\\frac{count(w_{i-1},w_i)+1}{count(w_{i-1})+V}$.\n",
        "That is, pretend we saw each word one more time than we did.\n",
        "\n",
        "You have two tasks here.\n",
        "\n",
        "First, compute the smoothed bigram and trigram models from the bigrams_freqDist and trigrams_freqDist you calculated above (use the unprocessed models). Second, using these smoothed models, predict the next 3 possible word sequences for testSent1, testSent2 and testSent3, using your smoothed models.\n",
        "\n",
        "As for example, for the string 'Raj has a' the answers can be as below: \n",
        "\n",
        "(1) Raj has a **beautiful red car**\n",
        "\n",
        "(2) Raj has a **charismatic magnetic personality**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAGB1_S8NThy"
      },
      "source": [
        "testSent1 = \"There was a sudden jerk, a terrific convulsion of the limbs; and there he\"\n",
        "testSent2 = \"They made room for the stranger, but he sat down\"\n",
        "testSent3 = \"The hungry and destitute situation of the infant orphan was duly reported by\""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqMGwF8Q6r9a",
        "outputId": "4dabce0b-e843-40d9-f09a-1dbcd4ee3dd8"
      },
      "source": [
        "def smoothing(unigrams_freqDist,bigrams_freqDist,trigrams_freqDist,sentence,ngram):\n",
        "  final_sentence=\"\"\n",
        "  probability_list=[]\n",
        "  if (ngram==\"bigram\"):\n",
        "    prev_word = sentence.split(\" \")[-1]\n",
        "    prev_word=(prev_word,) \n",
        "    next_words = []\n",
        "    for i in range(3):\n",
        "        probability = 0\n",
        "        append_word = \"\"\n",
        "        corpus_length = len(unigrams_freqDist.keys())                                    \n",
        "        for word, word_count in unigrams_freqDist.items():\n",
        "            current_word_probability = 0\n",
        "            previous_word_probability = 0 \n",
        "            if (prev_word[0], word[0]) in bigrams_freqDist.keys():\n",
        "                previous_word_probability = bigrams_freqDist[(prev_word[0],word[0])]\n",
        "            else:\n",
        "                previous_word_probability=0\n",
        "            current_word_probability = (previous_word_probability+1)/(word_count+corpus_length)\n",
        "            if  (current_word_probability > probability):\n",
        "                probability = current_word_probability\n",
        "                append_word = word\n",
        "        final_sentence=final_sentence+\" \"+append_word[0]\n",
        "        prev_word = append_word\n",
        "        probability_list.append(probability)\n",
        "  else:\n",
        "    previous_word = (sentence.split(\" \")[-1],)\n",
        "    prev_previous_word = (sentence.split(\" \")[-2],) \n",
        "    next_words = []\n",
        "    final_sentence = \"\"\n",
        "    corpus_length = len(unigrams_freqDist.keys()) \n",
        "    for _ in range(3):\n",
        "        probability = 0\n",
        "        append_word = \"\"                                        \n",
        "        for word, word_count in unigrams_freqDist.items():   \n",
        "            current_probability,previous_prev_count,previous_count = 0,0,0\n",
        "            if (prev_previous_word[0],previous_word[0],word[0]) in trigrams_freqDist.keys():\n",
        "                previous_prev_count = trigrams_freqDist[(prev_previous_word[0],previous_word[0],word[0])]\n",
        "            else:\n",
        "              previous_count = 0\n",
        "            if (prev_previous_word[0], previous_word[0]) in bigrams_freqDist.keys():\n",
        "                previous_count = bigrams_freqDist[(prev_previous_word[0],previous_word[0])]\n",
        "            else:\n",
        "              previous_count = 0  \n",
        "            current_probability = (previous_prev_count+1)/(previous_count + corpus_length)\n",
        "            if(current_probability>probability):\n",
        "                probability = current_probability\n",
        "                append_word = word\n",
        "        final_sentence=final_sentence+\" \"+append_word[0]\n",
        "        next_words.append(append_word)\n",
        "        prev_previous_word = previous_word\n",
        "        previous_word = append_word\n",
        "        probability_list.append(probability)\n",
        "  final_sentence=sentence+final_sentence\n",
        "  return final_sentence,probability_list\n",
        "\n",
        "predSent1_bigram,probability_bigram1 = smoothing(unigrams_freqDist,bigrams_freqDist,trigrams_freqDist,testSent1,\"bigram\")\n",
        "predSent2_bigram,probability_bigram2 = smoothing(unigrams_freqDist,bigrams_freqDist,trigrams_freqDist,testSent2,\"bigram\")\n",
        "predSent3_bigram,probability_bigram2 = smoothing(unigrams_freqDist,bigrams_freqDist,trigrams_freqDist,testSent3,\"bigram\")\n",
        "predSent1_trigram,probability_trigram1 = smoothing(unigrams_freqDist,bigrams_freqDist,trigrams_freqDist,testSent1,\"trigram\")\n",
        "predSent2_trigram,probability_trigram1 = smoothing(unigrams_freqDist,bigrams_freqDist,trigrams_freqDist,testSent2,\"trigram\")\n",
        "predSent3_trigram,probability_trigram1 = smoothing(unigrams_freqDist,bigrams_freqDist,trigrams_freqDist,testSent3,\"trigram\")\n",
        "\n",
        "print(\"Test Sentence:\",testSent1)\n",
        "print(\"Predicted Sentence using Bigram:\",predSent1_bigram)\n",
        "print(\"Predicted Sentence using Trigram:\",predSent1_trigram)\n",
        "print(\"-\"*100)\n",
        "print(\"-\"*100,\"\\n\")\n",
        "print(\"Test Sentence:\",testSent2)\n",
        "print(\"Predicted Sentence using Bigram:\",predSent2_bigram)\n",
        "print(\"Predicted Sentence using Trigram:\",predSent2_trigram)\n",
        "print(\"-\"*100)\n",
        "print(\"-\"*100,\"\\n\")\n",
        "print(\"Test Sentence:\",testSent3)\n",
        "print(\"Predicted Sentence using Bigram:\",predSent3_bigram)\n",
        "print(\"Predicted Sentence using Trigram:\",predSent3_trigram)\n",
        "print(\"-\"*100)\n",
        "print(\"-\"*100)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Sentence: There was a sudden jerk, a terrific convulsion of the limbs; and there he\n",
            "Predicted Sentence using Bigram: There was a sudden jerk, a terrific convulsion of the limbs; and there he had been thrown\n",
            "Predicted Sentence using Trigram: There was a sudden jerk, a terrific convulsion of the limbs; and there he sat down to\n",
            "----------------------------------------------------------------------------------------------------\n",
            "---------------------------------------------------------------------------------------------------- \n",
            "\n",
            "Test Sentence: They made room for the stranger, but he sat down\n",
            "Predicted Sentence using Bigram: They made room for the stranger, but he sat down the old gentleman\n",
            "Predicted Sentence using Trigram: They made room for the stranger, but he sat down to his heels\n",
            "----------------------------------------------------------------------------------------------------\n",
            "---------------------------------------------------------------------------------------------------- \n",
            "\n",
            "Test Sentence: The hungry and destitute situation of the infant orphan was duly reported by\n",
            "Predicted Sentence using Bigram: The hungry and destitute situation of the infant orphan was duly reported by the old gentleman\n",
            "Predicted Sentence using Trigram: The hungry and destitute situation of the infant orphan was duly reported by the side of\n",
            "----------------------------------------------------------------------------------------------------\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdteGt-ta0pt",
        "outputId": "6073965a-b519-4f19-e914-63897ddf72a5"
      },
      "source": [
        "actualSent1='There was a sudden jerk, a terrific convulsion of the limbs; and there he hung, with'\n",
        "actualSent2='They made room for the stranger, but he sat down in the furthest'\n",
        "actualSent3='The hungry and destitute situation of the infant orphan was duly reported by the workhouse authorities'\n",
        "\n",
        "print(\"Test Sentence:\",testSent1)\n",
        "print(\"Predicted Sentence using Bigram:\",predSent1_bigram)\n",
        "print(\"Predicted Sentence using Trigram:\",predSent1_trigram)\n",
        "print(\"Actual Sentence:\",actualSent1)\n",
        "print(\"-\"*100)\n",
        "print(\"-\"*100,\"\\n\")\n",
        "print(\"Test Sentence:\",testSent2)\n",
        "print(\"Predicted Sentence using Bigram:\",predSent2_bigram)\n",
        "print(\"Predicted Sentence using Trigram:\",predSent2_trigram)\n",
        "print(\"Actual Sentence:\",actualSent2)\n",
        "print(\"-\"*100)\n",
        "print(\"-\"*100,\"\\n\")\n",
        "print(\"Test Sentence:\",testSent3)\n",
        "print(\"Predicted Sentence using Bigram:\",predSent3_bigram)\n",
        "print(\"Predicted Sentence using Trigram:\",predSent3_trigram)\n",
        "print(\"Actual Sentence:\",actualSent3)\n",
        "print(\"-\"*100)\n",
        "print(\"-\"*100)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Sentence: There was a sudden jerk, a terrific convulsion of the limbs; and there he\n",
            "Predicted Sentence using Bigram: There was a sudden jerk, a terrific convulsion of the limbs; and there he had been thrown\n",
            "Predicted Sentence using Trigram: There was a sudden jerk, a terrific convulsion of the limbs; and there he sat down to\n",
            "Actual Sentence: There was a sudden jerk, a terrific convulsion of the limbs; and there he hung, with\n",
            "----------------------------------------------------------------------------------------------------\n",
            "---------------------------------------------------------------------------------------------------- \n",
            "\n",
            "Test Sentence: They made room for the stranger, but he sat down\n",
            "Predicted Sentence using Bigram: They made room for the stranger, but he sat down the old gentleman\n",
            "Predicted Sentence using Trigram: They made room for the stranger, but he sat down to his heels\n",
            "Actual Sentence: They made room for the stranger, but he sat down in the furthest\n",
            "----------------------------------------------------------------------------------------------------\n",
            "---------------------------------------------------------------------------------------------------- \n",
            "\n",
            "Test Sentence: The hungry and destitute situation of the infant orphan was duly reported by\n",
            "Predicted Sentence using Bigram: The hungry and destitute situation of the infant orphan was duly reported by the old gentleman\n",
            "Predicted Sentence using Trigram: The hungry and destitute situation of the infant orphan was duly reported by the side of\n",
            "Actual Sentence: The hungry and destitute situation of the infant orphan was duly reported by the workhouse authorities\n",
            "----------------------------------------------------------------------------------------------------\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxfeaacTdO6h"
      },
      "source": [
        "Check the presence of these sentences in the original corpus at https://www.gutenberg.org/files/730/730-0.txt . How did your smoothed models perform in comparison to the original sentences? Compare them below.\n",
        "\n",
        "Did you notice something special about testSent3, in comparison to testSent1 and testSent2? If yes, what is it? Can you explain it?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFMkW9hKecxK"
      },
      "source": [
        "  - - - - - - - - - -\n",
        "Yes, the prediction of testsSent3 is special in ways compared to testSent1 and testSent2. The first two test sentences have incorrectly predicted 3-words, while the prediction from testSent3 has correctly predicted word i.e,  \"the\" which matches the actual sentence, which has been predicted by both the smoothed bigram and trigram models.\n",
        "\n",
        "The third sentence, predictions etc is as follows\n",
        "\n",
        "*   Test Sentence: The hungry and destitute situation of the infant orphan was duly reported by\n",
        "*   Predicted Sentence using Bigram: The hungry and destitute situation of the infant orphan was duly reported by the old gentleman\n",
        "*   Predicted Sentence using Trigram: The hungry and destitute situation of the infant orphan was duly reported by the side of\n",
        "*   Actual Sentence: The hungry and destitute situation of the infant orphan was duly reported by the workhouse authorities\n",
        "\n",
        "This could be due to various reasons,\n",
        "\n",
        "*   The incorrectly predicted words is due to the fact that, the model purely predicts based on the frequency of occuring of the particular bigram or trigram words, which isn't totally accurate, as the context of the sentence hasn't been taken into account.\n",
        "*   The first two sentences are not in the corpus, i.e, not in the chpaters I to X, hence were inccorectly predicted, while testSent3 is present in chapter VI and hence had an edge over the other two test sentences.\n",
        "\n",
        "   - - - - - - - - - -"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Svrn1MAp0abI"
      },
      "source": [
        "def bigram_perplexity(unigrams_freqDist,bigrams_freqDist,sentence):\n",
        "    probability = 1\n",
        "    words = sentence.split(\" \")\n",
        "    corpus_length = len(unigrams_freqDist.keys())\n",
        "    for i in range(1,len(words)):\n",
        "        word = words[i]\n",
        "        prev_word = words[i-1]\n",
        "        count_current = 0\n",
        "        if prev_word in unigrams_freqDist.keys():\n",
        "            count_current = unigrams_freqDist[prev_word] \n",
        "        count_prev = 0\n",
        "        if (prev_word,word) in bigrams_freqDist.keys():\n",
        "                    count_prev = bigrams_freqDist[(prev_word,word)]\n",
        "        probability = probability*(count_prev+1)/(count_current + corpus_length)\n",
        "    return ((probability)**(-1/(len(words))))\n",
        "\n",
        "def trigram_perplexity(unigrams_freqDist,bigrams_freqDist,trigrams_freqDist,sentence):\n",
        "    prob = 1\n",
        "    words = sentence.split(\" \")\n",
        "    corpus_length = len(unigrams_freqDist.keys())\n",
        "    for i in range(2,len(words)):\n",
        "            word = words[i]\n",
        "            prev_word = words[i-1]\n",
        "            prev_prev_word = words[i-2]\n",
        "            count_word = 0\n",
        "            if (prev_word,word) in bigrams_freqDist.keys():\n",
        "                count_word = bigrams_freqDist[(prev_word,word)]\n",
        "            count_prev = 0\n",
        "            if (prev_prev_word,prev_word,word) in trigrams_freqDist.keys():\n",
        "                count_prev = trigrams_freqDist[(prev_prev_word,prev_word,word)]\n",
        "            prob = prob * (count_prev+1)/(count_word + len(unigrams_freqDist.keys()))\n",
        "    return (prob)**(-1/(len(words))) "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tni9-mzl0BsR"
      },
      "source": [
        "def perplexity(ngram,sentence,unigrams_freqDist,bigrams_freqDist,trigrams_freqDist):\n",
        "    if ngram == \"bigram\": \n",
        "        return bigram_perplexity(unigrams_freqDist,bigrams_freqDist,sentence)          \n",
        "    else:\n",
        "        return trigram_perplexity(unigrams_freqDist,bigrams_freqDist,trigrams_freqDist,sentence)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVBMcaAJXR9S"
      },
      "source": [
        "Which of the three models you generated above (unigram, bigram, trigram) is better in terms of **perplexity**, for the three test sentences (unseen data)? Write a piece of code to justify your answer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ustyxrec72vU",
        "outputId": "dfe6fb7a-12c9-423d-845d-64c2d75c4022"
      },
      "source": [
        "print(\"For the Test Sentence:\",testSent1)\n",
        "print(\"-\"*100)\n",
        "print(\"Bigram Perplexity:\",perplexity(\"bigram\",predSent1_bigram,unigrams_freqDist,bigrams_freqDist,trigrams_freqDist))\n",
        "print(\"Trigram Perplexity:\",perplexity(\"trigram\",predSent1_trigram,unigrams_freqDist,bigrams_freqDist,trigrams_freqDist))\n",
        "print()\n",
        "print(\"For the Test Sentence:\",testSent2)\n",
        "print(\"-\"*100)\n",
        "print(\"Bigram Perplexity:\",perplexity(\"bigram\",predSent2_bigram,unigrams_freqDist,bigrams_freqDist,trigrams_freqDist))\n",
        "print(\"Trigram Perplexity:\",perplexity(\"trigram\",predSent2_bigram,unigrams_freqDist,bigrams_freqDist,trigrams_freqDist))\n",
        "print()\n",
        "print(\"For the Test Sentence:\",testSent3)\n",
        "print(\"-\"*100)\n",
        "print(\"Bigram Perplexity:\",perplexity(\"bigram\",predSent3_bigram,unigrams_freqDist,bigrams_freqDist,trigrams_freqDist))\n",
        "print(\"Trigram Perplexity:\",perplexity(\"trigram\",predSent3_bigram,unigrams_freqDist,bigrams_freqDist,trigrams_freqDist))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For the Test Sentence: There was a sudden jerk, a terrific convulsion of the limbs; and there he\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Bigram Perplexity: 683.4294521913172\n",
            "Trigram Perplexity: 1270.5638566733828\n",
            "\n",
            "For the Test Sentence: They made room for the stranger, but he sat down\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Bigram Perplexity: 461.900383304322\n",
            "Trigram Perplexity: 839.6618142160153\n",
            "\n",
            "For the Test Sentence: The hungry and destitute situation of the infant orphan was duly reported by\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Bigram Perplexity: 533.8877406497093\n",
            "Trigram Perplexity: 759.5930117753347\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ccnr-pfkEetE"
      },
      "source": [
        "  - - - - - - - - - -\n",
        "To comapre the models, we use the evaluation metric, perplexity which is defined as follows,\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "PP(W) = (P(w1,w2,w3...wN))^-(1/N), N is the number of words in the corpus\n",
        "```\n",
        "Higher probability implies lower perplexity, and thus making the model a better one\n",
        "\n",
        "From the perplexity score values for the three sentences,  perplexity score using trigram model is higher in all the cases [1270.825,839.827,759.747]. The probabilities in the case of trigram model is less compared to the bigram model. For the given corpus, bigram model predicts with higher probability than trigram model, and hence a better model.\n",
        "\n",
        "However,a trigram model predicts more accurately when comapred to a bigram model, as more set of words are taken into consideration. But in this case bigram model makes a better fit than trigram model.\n",
        "   - - - - - - - - - -"
      ]
    }
  ]
}